https://www.virtualbox.org/changeset/41660/vbox/
Message: IPRT/r0drv/Linux: make it work with Linux 3.5 kernels

Index: /trunk/src/VBox/Runtime/r0drv/linux/the-linux-kernel.h
===================================================================
--- trunk/src/VBox/Runtime/r0drv/linux/the-linux-kernel.h	(revision 39841)
+++ trunk/src/VBox/Runtime/r0drv/linux/the-linux-kernel.h	(revision 41660)
@@ -243,5 +243,5 @@
 # if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 0)
 #  ifdef VM_ACCOUNT
-#   define MY_DO_MUNMAP(a,b,c) do_munmap(a, b, c, 0) /* should it be 1 or 0? */
+#   define USE_RHEL4_MUNMAP
 #  endif
 # endif
@@ -269,8 +269,4 @@
 # endif  /* !RT_ARCH_AMD64 */
 #endif /* !NO_REDHAT_HACKS */
-
-#ifndef MY_DO_MUNMAP
-# define MY_DO_MUNMAP(a,b,c) do_munmap(a, b, c)
-#endif
 
 #ifndef MY_CHANGE_PAGE_ATTR
Index: /trunk/src/VBox/Runtime/r0drv/linux/memobj-r0drv-linux.c
===================================================================
--- trunk/src/VBox/Runtime/r0drv/linux/memobj-r0drv-linux.c	(revision 41116)
+++ trunk/src/VBox/Runtime/r0drv/linux/memobj-r0drv-linux.c	(revision 41660)
@@ -101,4 +101,5 @@
 {
     /** @todo fix rtR0ProcessToLinuxTask!! */
+    /** @todo many (all?) callers currently assume that we return 'current'! */
     return R0Process == RTR0ProcHandleSelf() ? current : NULL;
 }
@@ -166,4 +167,105 @@
             return fKernel ? MY_PAGE_KERNEL_EXEC    : PAGE_SHARED_EXEC;
     }
+}
+
+
+/**
+ * Worker for rtR0MemObjNativeReserveUser and rtR0MemObjNativerMapUser that creates
+ * an empty user space mapping.
+ *
+ * We acquire the mmap_sem of the task!
+ *
+ * @returns Pointer to the mapping.
+ *          (void *)-1 on failure.
+ * @param   R3PtrFixed  (RTR3PTR)-1 if anywhere, otherwise a specific location.
+ * @param   cb          The size of the mapping.
+ * @param   uAlignment  The alignment of the mapping.
+ * @param   pTask       The Linux task to create this mapping in.
+ * @param   fProt       The RTMEM_PROT_* mask.
+ */
+static void *rtR0MemObjLinuxDoMmap(RTR3PTR R3PtrFixed, size_t cb, size_t uAlignment, struct task_struct *pTask, unsigned fProt)
+{
+    unsigned fLnxProt;
+    unsigned long ulAddr;
+
+    Assert((pTask == current)); /* do_mmap */
+
+    /*
+     * Convert from IPRT protection to mman.h PROT_ and call do_mmap.
+     */
+    fProt &= (RTMEM_PROT_NONE | RTMEM_PROT_READ | RTMEM_PROT_WRITE | RTMEM_PROT_EXEC);
+    if (fProt == RTMEM_PROT_NONE)
+        fLnxProt = PROT_NONE;
+    else
+    {
+        fLnxProt = 0;
+        if (fProt & RTMEM_PROT_READ)
+            fLnxProt |= PROT_READ;
+        if (fProt & RTMEM_PROT_WRITE)
+            fLnxProt |= PROT_WRITE;
+        if (fProt & RTMEM_PROT_EXEC)
+            fLnxProt |= PROT_EXEC;
+    }
+
+    if (R3PtrFixed != (RTR3PTR)-1)
+    {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 5, 0)
+        ulAddr = vm_mmap(NULL, R3PtrFixed, cb, fLnxProt, MAP_SHARED | MAP_ANONYMOUS | MAP_FIXED, 0);
+#else
+        down_write(&pTask->mm->mmap_sem);
+        ulAddr = do_mmap(NULL, R3PtrFixed, cb, fLnxProt, MAP_SHARED | MAP_ANONYMOUS | MAP_FIXED, 0);
+        up_write(&pTask->mm->mmap_sem);
+#endif
+    }
+    else
+    {
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 5, 0)
+        ulAddr = vm_mmap(NULL, 0, cb, fLnxProt, MAP_SHARED | MAP_ANONYMOUS, 0);
+#else
+        down_write(&pTask->mm->mmap_sem);
+        ulAddr = do_mmap(NULL, 0, cb, fLnxProt, MAP_SHARED | MAP_ANONYMOUS, 0);
+        up_write(&pTask->mm->mmap_sem);
+#endif
+        if (    !(ulAddr & ~PAGE_MASK)
+            &&  (ulAddr & (uAlignment - 1)))
+        {
+            /** @todo implement uAlignment properly... We'll probably need to make some dummy mappings to fill
+             * up alignment gaps. This is of course complicated by fragmentation (which we might have cause
+             * ourselves) and further by there begin two mmap strategies (top / bottom). */
+            /* For now, just ignore uAlignment requirements... */
+        }
+    }
+
+
+    if (ulAddr & ~PAGE_MASK) /* ~PAGE_MASK == PAGE_OFFSET_MASK */
+        return (void *)-1;
+    return (void *)ulAddr;
+}
+
+
+/**
+ * Worker that destroys a user space mapping.
+ * Undoes what rtR0MemObjLinuxDoMmap did.
+ *
+ * We acquire the mmap_sem of the task!
+ *
+ * @param   pv          The ring-3 mapping.
+ * @param   cb          The size of the mapping.
+ * @param   pTask       The Linux task to destroy this mapping in.
+ */
+static void rtR0MemObjLinuxDoMunmap(void *pv, size_t cb, struct task_struct *pTask)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 5, 0)
+    Assert(pTask == current);
+    vm_munmap((unsigned long)pv, cb);
+#elif defined(USE_RHEL4_MUNMAP)
+    down_write(&pTask->mm->mmap_sem);
+    do_munmap(pTask->mm, (unsigned long)pv, cb, 0); /* should it be 1 or 0? */
+    up_write(&pTask->mm->mmap_sem);
+#else
+    down_write(&pTask->mm->mmap_sem);
+    do_munmap(pTask->mm, (unsigned long)pv, cb);
+    up_write(&pTask->mm->mmap_sem);
+#endif
 }
 
@@ -424,5 +526,5 @@
 
 /**
- * Undos what rtR0MemObjLinuxVMap() did.
+ * Undoes what rtR0MemObjLinuxVMap() did.
  *
  * @param   pMemLnx     The linux memory object.
@@ -492,9 +594,5 @@
                 Assert(pTask);
                 if (pTask && pTask->mm)
-                {
-                    down_write(&pTask->mm->mmap_sem);
-                    MY_DO_MUNMAP(pTask->mm, (unsigned long)pMemLnx->Core.pv, pMemLnx->Core.cb);
-                    up_write(&pTask->mm->mmap_sem);
-                }
+                    rtR0MemObjLinuxDoMunmap(pMemLnx->Core.pv, pMemLnx->Core.cb, pTask);
             }
             else
@@ -517,9 +615,5 @@
                 Assert(pTask);
                 if (pTask && pTask->mm)
-                {
-                    down_write(&pTask->mm->mmap_sem);
-                    MY_DO_MUNMAP(pTask->mm, (unsigned long)pMemLnx->Core.pv, pMemLnx->Core.cb);
-                    up_write(&pTask->mm->mmap_sem);
-                }
+                    rtR0MemObjLinuxDoMunmap(pMemLnx->Core.pv, pMemLnx->Core.cb, pTask);
             }
             else
@@ -1120,60 +1214,4 @@
 
 
-/**
- * Worker for rtR0MemObjNativeReserveUser and rtR0MemObjNativerMapUser that creates
- * an empty user space mapping.
- *
- * The caller takes care of acquiring the mmap_sem of the task.
- *
- * @returns Pointer to the mapping.
- *          (void *)-1 on failure.
- * @param   R3PtrFixed  (RTR3PTR)-1 if anywhere, otherwise a specific location.
- * @param   cb          The size of the mapping.
- * @param   uAlignment  The alignment of the mapping.
- * @param   pTask       The Linux task to create this mapping in.
- * @param   fProt       The RTMEM_PROT_* mask.
- */
-static void *rtR0MemObjLinuxDoMmap(RTR3PTR R3PtrFixed, size_t cb, size_t uAlignment, struct task_struct *pTask, unsigned fProt)
-{
-    unsigned fLnxProt;
-    unsigned long ulAddr;
-
-    /*
-     * Convert from IPRT protection to mman.h PROT_ and call do_mmap.
-     */
-    fProt &= (RTMEM_PROT_NONE | RTMEM_PROT_READ | RTMEM_PROT_WRITE | RTMEM_PROT_EXEC);
-    if (fProt == RTMEM_PROT_NONE)
-        fLnxProt = PROT_NONE;
-    else
-    {
-        fLnxProt = 0;
-        if (fProt & RTMEM_PROT_READ)
-            fLnxProt |= PROT_READ;
-        if (fProt & RTMEM_PROT_WRITE)
-            fLnxProt |= PROT_WRITE;
-        if (fProt & RTMEM_PROT_EXEC)
-            fLnxProt |= PROT_EXEC;
-    }
-
-    if (R3PtrFixed != (RTR3PTR)-1)
-        ulAddr = do_mmap(NULL, R3PtrFixed, cb, fLnxProt, MAP_SHARED | MAP_ANONYMOUS | MAP_FIXED, 0);
-    else
-    {
-        ulAddr = do_mmap(NULL, 0, cb, fLnxProt, MAP_SHARED | MAP_ANONYMOUS, 0);
-        if (    !(ulAddr & ~PAGE_MASK)
-            &&  (ulAddr & (uAlignment - 1)))
-        {
-            /** @todo implement uAlignment properly... We'll probably need to make some dummy mappings to fill
-             * up alignment gaps. This is of course complicated by fragmentation (which we might have cause
-             * ourselves) and further by there begin two mmap strategies (top / bottom). */
-            /* For now, just ignore uAlignment requirements... */
-        }
-    }
-    if (ulAddr & ~PAGE_MASK) /* ~PAGE_MASK == PAGE_OFFSET_MASK */
-        return (void *)-1;
-    return (void *)ulAddr;
-}
-
-
 DECLHIDDEN(int) rtR0MemObjNativeReserveUser(PPRTR0MEMOBJINTERNAL ppMem, RTR3PTR R3PtrFixed, size_t cb, size_t uAlignment, RTR0PROCESS R0Process)
 {
@@ -1193,7 +1231,5 @@
      * Let rtR0MemObjLinuxDoMmap do the difficult bits.
      */
-    down_write(&pTask->mm->mmap_sem);
     pv = rtR0MemObjLinuxDoMmap(R3PtrFixed, cb, uAlignment, pTask, RTMEM_PROT_NONE);
-    up_write(&pTask->mm->mmap_sem);
     if (pv == (void *)-1)
         return VERR_NO_MEMORY;
@@ -1202,7 +1238,5 @@
     if (!pMemLnx)
     {
-        down_write(&pTask->mm->mmap_sem);
-        MY_DO_MUNMAP(pTask->mm, (unsigned long)pv, cb);
-        up_write(&pTask->mm->mmap_sem);
+        rtR0MemObjLinuxDoMunmap(pv, cb, pTask);
         return VERR_NO_MEMORY;
     }
@@ -1391,5 +1425,4 @@
          */
         void *pv;
-        down_write(&pTask->mm->mmap_sem);
         pv = rtR0MemObjLinuxDoMmap(R3PtrFixed, pMemLnxToMap->Core.cb, uAlignment, pTask, fProt);
         if (pv != (void *)-1)
@@ -1404,5 +1437,7 @@
             size_t          iPage;
 
-            rc = 0;
+            down_write(&pTask->mm->mmap_sem);
+
+            rc = VINF_SUCCESS;
             if (pMemLnxToMap->cPages)
             {
@@ -1486,11 +1521,12 @@
                 }
             }
-            if (!rc)
-            {
-                up_write(&pTask->mm->mmap_sem);
+
+            up_write(&pTask->mm->mmap_sem);
+
+            if (RT_SUCCESS(rc))
+            {
 #ifdef VBOX_USE_PAE_HACK
                 __free_page(pDummyPage);
 #endif
-
                 pMemLnx->Core.pv = pv;
                 pMemLnx->Core.u.Mapping.R0Process = R0Process;
@@ -1502,7 +1538,6 @@
              * Bail out.
              */
-            MY_DO_MUNMAP(pTask->mm, (unsigned long)pv, pMemLnxToMap->Core.cb);
-        }
-        up_write(&pTask->mm->mmap_sem);
+            rtR0MemObjLinuxDoMunmap(pv, pMemLnxToMap->Core.cb, pTask);
+        }
         rtR0MemObjDelete(&pMemLnx->Core);
     }
